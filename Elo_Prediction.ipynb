{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14501a60",
   "metadata": {},
   "source": [
    "# Predicting Elo Scores Based on Player Performance through Board States"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a366de8",
   "metadata": {},
   "source": [
    "## Team Members\n",
    "### Alberto Garcia Roberto Palacios Logan Druley"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed3e01",
   "metadata": {},
   "source": [
    "Elo rankings are scores assigned to chess players based on their performance against opponents. Elo scores are calculated by subtracting a player's expected score from their actual score and then multiplying that result by a factor K that controls how fast ratings in a given system are meant to change. The result of this calculation is then added to the player's rating. The formula for this calculation is:\n",
    "$$\n",
    "R_A' = R_A + K(S_A - E_A)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f6bddc",
   "metadata": {},
   "source": [
    "However, a player's Elo rating takes a signficant amount of time to reveal itself. A player must take part in several games before their performance reveals their current skill level. Our plan is to take annotated chess games with stated Elo rankings for each player and train a transformer to read the board states of games. Based on the moves a player takes in relation to their opponent we want the transformer predict the Elo ranking of said player. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77c448fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking and installing required packages...\n",
      "============================================================\n",
      "Installing zstandard...\n",
      "Collecting zstandard\n",
      "  Downloading zstandard-0.25.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.6 MB)\n",
      "Installing collected packages: zstandard\n",
      "Successfully installed zstandard-0.25.0\n",
      "✓ zstandard installed successfully\n",
      "Installing tqdm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/home/rpalacios_csumb/source/bin/python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/home/rpalacios_csumb/source/bin/python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ tqdm installed successfully\n",
      "Installing numpy...\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-2.0.2\n",
      "✓ numpy installed successfully\n",
      "Installing pandas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/home/rpalacios_csumb/source/bin/python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from pandas) (2.0.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Installing collected packages: tzdata, pytz, pandas\n",
      "Successfully installed pandas-2.3.3 pytz-2025.2 tzdata-2025.3\n",
      "✓ pandas installed successfully\n",
      "Installing scikit-learn...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/home/rpalacios_csumb/source/bin/python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "Collecting scipy>=1.6.0\n",
      "  Using cached scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from scikit-learn) (2.0.2)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.2 scikit-learn-1.6.1 scipy-1.13.1 threadpoolctl-3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/home/rpalacios_csumb/source/bin/python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ scikit-learn installed successfully\n",
      "Installing chess...\n",
      "Collecting chess\n",
      "  Using cached chess-1.11.2.tar.gz (6.1 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Using legacy 'setup.py install' for chess, since package 'wheel' is not installed.\n",
      "Installing collected packages: chess\n",
      "    Running setup.py install for chess: started\n",
      "    Running setup.py install for chess: finished with status 'done'\n",
      "Successfully installed chess-1.11.2\n",
      "✓ chess installed successfully\n",
      "Installing matplotlib...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/home/rpalacios_csumb/source/bin/python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from matplotlib) (25.0)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (321 kB)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Using cached kiwisolver-1.4.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Collecting importlib-resources>=3.2.0\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.60.2-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)\n",
      "Collecting pillow>=8\n",
      "  Using cached pillow-11.3.0-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.23.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.60.2 importlib-resources-6.5.2 kiwisolver-1.4.7 matplotlib-3.9.4 pillow-11.3.0 pyparsing-3.2.5\n",
      "✓ matplotlib installed successfully\n",
      "Installing seaborn...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/home/rpalacios_csumb/source/bin/python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from seaborn) (3.9.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from seaborn) (2.3.3)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from seaborn) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.60.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.5)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (6.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pillow>=8 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from pandas>=1.2->seaborn) (2025.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.4->seaborn) (3.23.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/rpalacios_csumb/source/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "✓ seaborn installed successfully\n",
      "Installing tensorflow...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/home/rpalacios_csumb/source/bin/python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['/home/rpalacios_csumb/source/bin/python', '-m', 'pip', 'install', 'tensorflow']' died with <Signals.SIGKILL: 9>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m, in \u001b[0;36minstall_package\u001b[0;34m(package)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is already installed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m package \u001b[38;5;129;01min\u001b[39;00m required_packages:\n\u001b[0;32m---> 32\u001b[0m     \u001b[43minstall_package\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ All required packages are installed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36minstall_package\u001b[0;34m(package)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-m\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minstall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m installed successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib64/python3.9/subprocess.py:373\u001b[0m, in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cmd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    372\u001b[0m         cmd \u001b[38;5;241m=\u001b[39m popenargs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, cmd)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['/home/rpalacios_csumb/source/bin/python', '-m', 'pip', 'install', 'tensorflow']' died with <Signals.SIGKILL: 9>."
     ]
    }
   ],
   "source": [
    "# Install required dependencies if not already installed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package using pip.\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"✓ {package} is already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"✓ {package} installed successfully\")\n",
    "\n",
    "# Install required packages\n",
    "required_packages = [\n",
    "    \"zstandard\",\n",
    "    \"tqdm\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"scikit-learn\",\n",
    "    \"chess\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"tensorflow\",\n",
    "    \"pyarrow\",\n",
    "]\n",
    "\n",
    "print(\"Checking and installing required packages...\")\n",
    "print(\"=\" * 60)\n",
    "for package in required_packages:\n",
    "    install_package(package)\n",
    "print(\"=\" * 60)\n",
    "print(\"✓ All required packages are installed!\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02ee2267",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'zstandard'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, os\u001b[38;5;241m.\u001b[39mgetcwd())\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Import functions from preprocess_data.py\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocess_data\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m decompress_and_parse_pgn, create_datasets\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparser\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_pgn_stream, split_dataset\n",
      "File \u001b[0;32m~/elo-predictor/scripts/preprocess_data.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mzstandard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mzstd\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'zstandard'"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.insert(0, os.getcwd())\n",
    "\n",
    "# Import functions from preprocess_data.py\n",
    "from scripts.preprocess_data import decompress_and_parse_pgn, create_datasets\n",
    "from src.data.parser import parse_pgn_stream, split_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d70e4b6",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cea2f05",
   "metadata": {},
   "source": [
    "Here we start by decompressing and then processing 90 Mb worth of annotated games to prepare them for model training. The games are parsed into relevant metadata including the white and black player's Elo rating and also transforms game turns into FEN strings. This transformation allows use to process the entire board's state which in turn gives us the opportunity to use self-attention to discover the importance of each move in the larger context of the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90edbd24",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decompress_and_parse_pgn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/processed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Decompress and parse the PGN file\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m games \u001b[38;5;241m=\u001b[39m \u001b[43mdecompress_and_parse_pgn\u001b[49m(\n\u001b[1;32m      7\u001b[0m     input_path\u001b[38;5;241m=\u001b[39minput_file,\n\u001b[1;32m      8\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[1;32m      9\u001b[0m     max_games\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# Process all games (set to a number like 10000 for testing)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Create train/val/test splits\u001b[39;00m\n\u001b[1;32m     14\u001b[0m train_games, val_games, test_games \u001b[38;5;241m=\u001b[39m create_datasets(\n\u001b[1;32m     15\u001b[0m     games\u001b[38;5;241m=\u001b[39mgames,\n\u001b[1;32m     16\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     test_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m     20\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decompress_and_parse_pgn' is not defined"
     ]
    }
   ],
   "source": [
    "# Process the Lichess PGN file\n",
    "input_file = \"lichess_db_standard_rated_2013-01.pgn.zst\"\n",
    "output_dir = \"data/processed\"\n",
    "\n",
    "# Decompress and parse the PGN file\n",
    "games = decompress_and_parse_pgn(\n",
    "    input_path=input_file,\n",
    "    output_dir=output_dir,\n",
    "    max_games=None,  # Process all games (set to a number like 10000 for testing)\n",
    "    batch_size=1000\n",
    ")\n",
    "\n",
    "# Create train/val/test splits\n",
    "train_games, val_games, test_games = create_datasets(\n",
    "    games=games,\n",
    "    output_dir=output_dir,\n",
    "    train_ratio=0.8,\n",
    "    val_ratio=0.1,\n",
    "    test_ratio=0.1\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Processing complete!\")\n",
    "print(f\"  Total games: {len(games):,}\")\n",
    "print(f\"  Train: {len(train_games):,}\")\n",
    "print(f\"  Val: {len(val_games):,}\")\n",
    "print(f\"  Test: {len(test_games):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d259b10",
   "metadata": {},
   "source": [
    "## Baseline Model: LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c34b0a",
   "metadata": {},
   "source": [
    "Once we've processed the data and then seperated it into train, validation, and test sets, we can then begin to train our initial baseline model. For this purpose, we've chosen an LSTM model as it has the capability to capture relationships across time. However, the performance of an LSTM should be weak in comparison to that of a Transformer with self-attention. As such, this baseline will tell us whether or not we have produce a model with any kind of significant ability to predict Elo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ca479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:scripts.train_model:Train: 5000, Val: 500, Test: 500\n",
      "INFO:scripts.train_model:Creating dataloaders...\n",
      "INFO:scripts.train_model:Creating lstm model...\n",
      "INFO:scripts.train_model:Saved config to experiments/quick_test\\config_20251208_224153.json\n",
      "INFO:scripts.train_model:Starting training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data from previous cell...\n",
      "\n",
      "Full dataset: Train=96891, Val=12111, Test=12112\n",
      "Creating subset for quick test (5000 train, 500 val, 500 test)...\n",
      "Subset: Train=5000, Val=500, Test=500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 157/157 [09:13<00:00,  3.53s/it, loss=0.0106, mae=0.0816]\n",
      "Validating: 100%|██████████| 16/16 [00:19<00:00,  1.23s/it, loss=0.0101] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train MAE: 0.081552 | Val MAE: 0.081105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 157/157 [08:53<00:00,  3.40s/it, loss=0.0105, mae=0.0811]\n",
      "Validating: 100%|██████████| 16/16 [00:19<00:00,  1.23s/it, loss=0.00991]\n",
      "INFO:scripts.train_model:Evaluating on test set...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | Train MAE: 0.081064 | Val MAE: 0.080164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 16/16 [00:20<00:00,  1.25s/it, loss=0.0111]\n",
      "INFO:scripts.train_model:Test MAE: 0.085656\n",
      "INFO:scripts.train_model:Test MAE (Elo): 171.30\n",
      "INFO:scripts.train_model:\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "QUICK TEST RESULTS\n",
      "============================================================\n",
      "Test MAE (normalized): 0.085656\n",
      "Test RMSE (normalized): 0.105208\n",
      "Test MAE (Elo points): 171.30\n",
      "Test RMSE (Elo points): 210.42\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Train LSTM model with subset of data\n",
    "from scripts.train_model import train_model, load_dataset\n",
    "import random\n",
    "\n",
    "# Check if train_games exists (from previous cell), otherwise load from disk\n",
    "if 'train_games' not in globals() or train_games is None:\n",
    "    print(\"Loading dataset from disk...\")\n",
    "    data_dir = \"data/processed\"\n",
    "    train_games, val_games, test_games = load_dataset(data_dir)\n",
    "    \n",
    "    if train_games is None:\n",
    "        raise ValueError(\"No data found! Please run the data processing cell (Cell 4) first.\")\n",
    "else:\n",
    "    print(\"Using data from previous cell...\")\n",
    "\n",
    "print(f\"\\nFull dataset: Train={len(train_games)}, Val={len(val_games)}, Test={len(test_games)}\")\n",
    "print(\"Creating subset for quick test (5000 train, 500 val, 500 test)...\")\n",
    "\n",
    "random.seed(42)\n",
    "train_games = random.sample(train_games, min(5000, len(train_games)))\n",
    "val_games = random.sample(val_games, min(500, len(val_games)))\n",
    "test_games = random.sample(test_games, min(500, len(test_games)))\n",
    "\n",
    "print(f\"Subset: Train={len(train_games)}, Val={len(val_games)}, Test={len(test_games)}\\n\")\n",
    "\n",
    "# Train the model with the subset\n",
    "trainer, history, test_metrics = train_model(\n",
    "    train_games=train_games,\n",
    "    val_games=val_games,\n",
    "    test_games=test_games,\n",
    "    model=\"lstm\",\n",
    "    batch_size=32,\n",
    "    epochs=2,  # Quick test: 2 epochs (~30 min)\n",
    "    lr=1e-3,\n",
    "    dropout=0.1,\n",
    "    embedding_dim=128,\n",
    "    seed=42,\n",
    "    output_dir=\"experiments/quick_test\",\n",
    "    early_stopping=False,\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"QUICK TEST RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Test MAE (normalized): {test_metrics['mae']:.6f}\")\n",
    "print(f\"Test RMSE (normalized): {test_metrics['rmse']:.6f}\")\n",
    "if 'mae_elo' in test_metrics:\n",
    "    print(f\"Test MAE (Elo points): {test_metrics['mae_elo']:.2f}\")\n",
    "    print(f\"Test RMSE (Elo points): {test_metrics['rmse_elo']:.2f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10a880",
   "metadata": {},
   "source": [
    "This initial run only encompasses portion of our data and despite only runing for 2 epoch, we can already see that the LSTM model performs within the 100-250 Elo range that we were looking for as a baseline. The +0.02 gap between RMSE and MAE suggests there no large outliers in this sample of our data but that might change when working with a larger dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430a9000",
   "metadata": {},
   "source": [
    "## Optimized Transformer Model Training\n",
    "\n",
    "Now we'll train the Transformer model using the optimized TensorFlow training script. This script includes:\n",
    "- Multi-GPU support with MirroredStrategy\n",
    "- Mixed precision training (FP16) for faster training\n",
    "- Streaming data loading with parallel processing\n",
    "- Checkpoint resumption (can resume from latest checkpoint)\n",
    "- Multiple checkpoint strategies (per-epoch, best, last)\n",
    "- Learning rate scheduling\n",
    "- TFLite model export\n",
    "\n",
    "The optimized training script matches the structure of `run_full_train_optimized.py` and provides significant performance improvements over the baseline training approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9f3fdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Dataset splits not found!\n",
      "Please run the data preprocessing cell (Cell 7) first.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Dataset splits not found at data/processed/dataset_splits.pkl",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Dataset splits not found!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run the data preprocessing cell (Cell 7) first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset splits not found at data/processed/dataset_splits.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTARTING OPTIMIZED TRANSFORMER TRAINING\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Dataset splits not found at data/processed/dataset_splits.pkl"
     ]
    }
   ],
   "source": [
    "# Train Transformer model with optimized TensorFlow script\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if data exists\n",
    "data_dir = Path(\"data/processed\")\n",
    "if not (data_dir / \"dataset_splits.pkl\").exists():\n",
    "    print(\"Error: Dataset splits not found!\")\n",
    "    print(\"Please run the data preprocessing cell (Cell 7) first.\")\n",
    "    raise FileNotFoundError(\"Dataset splits not found at data/processed/dataset_splits.pkl\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING OPTIMIZED TRANSFORMER TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Optional: Convert to Parquet for better performance (uncomment if desired)\n",
    "# print(\"Converting dataset to Parquet format...\")\n",
    "# from scripts.convert_to_parquet import main as convert_main\n",
    "# convert_main()\n",
    "# print()\n",
    "\n",
    "# Import and run the optimized training script\n",
    "print(\"Running optimized training script...\")\n",
    "print(\"(This will use the full dataset and may take several hours)\")\n",
    "print()\n",
    "\n",
    "# Run the training directly\n",
    "from scripts.train_transformer_optimized import main as train_main\n",
    "\n",
    "try:\n",
    "    train_main()\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"✓ Training completed successfully!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nCheckpoints saved to: checkpoints/\")\n",
    "    print(\"Best model: transformer_elo_optimized_best.keras\")\n",
    "    print(\"Final model: transformer_elo_optimized.keras\")\n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"✗ Training failed with error:\")\n",
    "    print(str(e))\n",
    "    print(\"=\" * 60)\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "source",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
